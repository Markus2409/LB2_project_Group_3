{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "yZhnjtdux7bn"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from requests.adapters import HTTPAdapter, Retry\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Configure retry strategy to handle temporary errors"
      ],
      "metadata": {
        "id": "4NVOPGcEMaNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retries = Retry(total=5, backoff_factor=0.25, status_forcelist=[500, 502, 503, 504])\n",
        "session = requests.Session()\n",
        "session.mount(\"https://\", HTTPAdapter(max_retries=retries))"
      ],
      "metadata": {
        "id": "XJN5V68ZyFW2"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Definition of batch size and the url from which recover the information that has to be filtered"
      ],
      "metadata": {
        "id": "-r6htUUEMniO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We define a basic URL for the search.\n",
        "# We look for all non-fragment reviewed human protein having a coiled-coil region in the first 100 residues.\n",
        "# The URL has been generated from the UniProtKB website, using the\n",
        "# Advanced search function.\n",
        "batch_size = 500\n",
        "url = \"https://rest.uniprot.org/uniprotkb/search?format=json&query=%28%28existence%3A1%29+AND+%28length%3A%5B40+TO+*%5D%29+AND+%28reviewed%3Atrue%29+AND+%28taxonomy_id%3A2759%29+AND+%28fragment%3Afalse%29+AND+%28ft_signal_exp%3A*%29%29&size=500\"\n",
        "\n",
        "def get_next_link(headers):\n",
        "    if \"Link\" in headers:\n",
        "        # The regular expression is used to extract the next link for pagination\n",
        "        re_next_link = re.compile(r'<(.+)>; rel=\"next\"')\n",
        "        match = re_next_link.match(headers[\"Link\"])\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "\n",
        "# This function actually retrieve the next data batch in the search.\n",
        "# The function act as an iterator, yielding the next result batch at every call\n",
        "# The function terminates after the last batch has been returned. In this case,\n",
        "# the next link will be None\n",
        "def get_batch(batch_url):\n",
        "    while batch_url:\n",
        "        # Run the API call\n",
        "        response = session.get(batch_url)\n",
        "        # Will raise an error if an error status code is obtained\n",
        "        response.raise_for_status()\n",
        "        # Get the total number of entries in the search\n",
        "        total = response.headers[\"x-total-results\"]\n",
        "        # Yield the response and the total number of entries\n",
        "        yield response, total\n",
        "        # Get the link to the API call for the next data batch\n",
        "        batch_url = get_next_link(response.headers)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r_QPsPaSCs9I"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Definition filters and fields that has to be inserted in tsv file"
      ],
      "metadata": {
        "id": "Tizc5709N1ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# All search criteria, except the location of the signal peptide shorter than 14 residues, and existence of the cleavage site, can be specified using the search URL\n",
        "# To only include proteins with signal peptide longer than 14 residues and with the existence of the, we define a filter function\n",
        "# It returns True if the entry passess the filter, False otherwise\n",
        "\n",
        "def filter_entry(entry):\n",
        "    # We iterate over the features of the entry\n",
        "    for feature in entry[\"features\"]:\n",
        "        # We only consider features of type Coiled coil\n",
        "        if feature[\"type\"] == \"Signal\":\n",
        "            # Check if the coiled-coil starts before position 100\n",
        "            if type(feature[\"location\"][\"end\"][\"value\"]) == int:\n",
        "              if feature[\"location\"][\"end\"][\"value\"] >= 14 and feature[\"description\"]==\"\":\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "# We set the name of the output file, we want TSV output\n",
        "output_file = \"positive_dataset.tsv\"\n",
        "\n",
        "# We define a function to better control the TSV format in output.\n",
        "# In particular, we run the API call requiring JSON format and build our own TSV file\n",
        "# The this aim, the following function extract and process specific fields from the JSON file\n",
        "organisms=[\"Metazoa\",\"Fungi\",\"Viridiplantae\"]\n",
        "def extract_fields(entry):\n",
        "    # We iterate over the features of the entry\n",
        "    for f in entry[\"features\"]:\n",
        "        # We only consider the signal fragment\n",
        "        if f[\"type\"] == \"Signal\":\n",
        "            # Check if the signal peptide is longer than 14 and description is empty:\n",
        "          if f[\"location\"][\"end\"][\"value\"] >= 14 and f[\"description\"]==\"\":\n",
        "            s=f[\"location\"][\"start\"][\"value\"]\n",
        "            e=f[\"location\"][\"end\"][\"value\"]\n",
        "            break\n",
        "    if entry[\"organism\"][\"lineage\"][1] in organisms: # Check if the kingdom is Metazoa, Fungi, or Viridiplantae; if not, assign \"Other\"\n",
        "      return (entry[\"primaryAccession\"], entry[\"organism\"][\"scientificName\"], entry[\"organism\"][\"lineage\"][1],entry[\"sequence\"][\"length\"],s,e)\n",
        "    else:\n",
        "      return (entry[\"primaryAccession\"], entry[\"organism\"][\"scientificName\"], \"Other\",entry[\"sequence\"][\"length\"],s,e)"
      ],
      "metadata": {
        "id": "1KT3NQnfN0ve"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Define dataset creation: apply filters, extract fields, and save results in a TSV file\n"
      ],
      "metadata": {
        "id": "ZZqB9twLOfoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(search_url, filter_function, extract_function, output_file_name):\n",
        "    filtered_json = []\n",
        "    n_total, n_filtered = 0, 0\n",
        "    # Run the API call in batches\n",
        "    for batch, total in get_batch(search_url):\n",
        "        # parse the JSON body of the response\n",
        "        batch_json = json.loads(batch.text)\n",
        "        # filter the entries\n",
        "        for entry in batch_json[\"results\"]:\n",
        "            n_total += 1\n",
        "            # Check if the entry passes the filter\n",
        "            if filter_function(entry):\n",
        "                n_filtered += 1\n",
        "                filtered_json.append(entry) # or filtered_json.append(extract_function(entry)) if you want optimize avoiding to save the entire json\n",
        "    print(n_total, n_filtered)\n",
        "    with open(output_file_name, \"w\") as ofs:\n",
        "        for entry in filtered_json:\n",
        "            # Extract the fields of interest\n",
        "            fields = extract_fields(entry)\n",
        "            # Print the fields in TSV format\n",
        "            print(*fields, sep=\"\\t\", file=ofs)\n",
        "        ofs.close"
      ],
      "metadata": {
        "id": "0iJeKL7zBXm7"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_dataset(url, filter_entry, extract_fields, output_file) #get tsv file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqoCod24BZ7R",
        "outputId": "8332ee41-de00-4f0e-d4f6-1c2c6837f15c"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2949 2932\n"
          ]
        }
      ]
    }
  ]
}